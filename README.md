# Automatic-Text-Summarization
This repo. contains exploratory analysis and multiple experiments using Transformers

Automatic Text Summarization (ATS) is essentially becoming an important and popular NLP based research area considering the amount of textual data including news, articles, magazine, stories, legal documents, reviews, research papers getting generated every second around the world. Many state-of-the-art (SOTA) models including Transfer Learning (TL), Reinforcement Learning (RL), transformer architecture and Pre-trained Language Models (PTLMs) were proposed and developed till date however the generated text are far behind the human generated summaries. We observed that researchers have given comparatively less focus on abstractive and hybrid approaches than the extractive approach. This paper dispenses a novel approach to generate diversified texts without compromising the quality. Also, we extract few new linguistic and statistical features for words and sentences. In this paper, we will refer Information Bottleneck principle, to do unsupervised extractive summarization using pretrained language models such as T5 (Text-to-Text Transfer Transformer) and BART (Bidirectional and Auto Regressive Transformer). An improved attention mechanism is applied to handle “out of vocabulary” (OOV) and rare words. In this work, training data for self-supervised abstractive summarization, is formed by making extractive summarized data as ground-truth. And also, the training data is augmented with synthetically corrupted summaries and add them in the target data set. During the training custom contrastive loss term is added so that model learns to ignore fact-based inconsistency. Then using this dataset T5 or BART are trained to generate abstractive and factually consistent summaries. The generated summary from the model is vetted against the human crafted reference summary to measure the performance of the model. Our method achieves promising result on the CNN/DailyMail dataset (0.436 ROUGE-1) very much at par with the state-of-the-art results however our model doesn’t depend on the availability of the reference summary for training. This is a clear differentiator of our solution when we compare with other available state-of-the-art approaches. 
